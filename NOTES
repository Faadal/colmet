============
CODING NOTES
============


Principe de fonctionnement
==========================

colmet a pour but de collecter des mesures des ressources utilisées par une tâche sur une plateforme de calcul.
Un usage typique est la supervision des tâches sur une grappe de calcul.

En gros actuellement colmet est composé:

  - d'une boucle principale
  - de métriques
  - de backends d'entrée et sortie

Boucle principale
-----------------

colmet/ui.py

Son principe consiste à récupérer une liste de métriques depuis un backend
d'entrée et de les fournir au backend de sortie. Ceci est fait en boucle. Cette
boucle est paramétrable en terme de nombre d'itérations et en terme de délai
entre chaque itération.

Il est aussi possible démarrer colmet en tant que démon.

Les métriques
-------------

colmet/metrics/base.py

Une métrique est un ensemble de compteurs et d'entêtes. Une métrique possède au
moins 4 entêtes (timestamp, nom d'hôte, numéro de tâche, nom de la métrique)

Les compteurs sont définies dans le tableau _counters et les entêtes dans le
tableau _headers.

Un compteur est défini par:
    - son nom
    - son type
    - la manière de la représenter
    - la manière d'accumuler le compteur
    - sa description

Une entête est définie par:
    - son nom
    - son type
    - la manière de la représenter


Type de compteur et/ou d'entête
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Par défaut, il existe plusieurs types:
  - UInt8, UInt16, UInt32, UInt64
  - String

Les types de base sont définis dans colmet/metrics/base.py. Il est possible de
définir un nouveau type dans la mesure où le type donné est transcodable avec
les fonctions struct.pack() et struct.unpack().

(!) Un type doit avoir une taille fixe.


Représentation d'un compteur et/ou entête
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Une représentation permet de définir la manière dont sera affichée la compteur
sur la sortie standard. il existe plusieurs représentations par défaut :

  - bytes        => la valeur est en octets
  - kbytes       => la valeur est en kilo-octets
  - mbytes       => la valeur est en mega-octets
  - ts_date      => la valeur est un timestamp a représenter sous forme de date
  - usec         => la valeur est en micro-secondes
  - nsec         => la valeur est en nano-secondes
  - count        => la valeur est un nombre
  - mbytes-usec  => la valeur est en mega-octets * micro-secondes
  - n/a          => inconnu


Fonction d'accumulation des compteurs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Une fonction d'accumulation permet de définir ce que colmet doit faire quand il
doit accumuler deux valeurs d'un compteur au fil du temps.

Par défault, il existe plusieurs fonctions d'accumulation:

  - 'add'  : x,y,coeff -> y if x == None else x if y == None else x + coeff * y
  - 'none' : x,y,coeff -> y if x == None else y
  - 'min'  : x,y,coeff -> y if x == None else x if y == None else x if x < y else y
  - 'max'  : x,y,coeff -> y if x == None else x if y == None else x if x >= y else y


Les backends entrée/sorties
---------------------------

colmet/backends/__init__.py
colmet/backends/base.py

Un backend d'entrée/sortie doit à minima fournir une fonction pull/push qui
permet de récupérer/envoyer une liste de métriques.

Les backends existants :
  - taskstats
  - stdout
  - zeromq
  - zeromq_aggregator
  - hdf5
  - rrd

**taskstats**: permet de récuperer les compteurs taskstats fournis par le noyau
linux à partir d'un numéro de groupe de tâches (tgid,pid), d'un numéro de tâche
(tid), ou d'un chemin cgroup/cpuset. Ce backend utilise la métrique
'colmet.metrics.taskstats_default'

**stdout**: affiche les métriques sur la sortie standard. Utilisé
principalement pour le débuggage.

**zeromq**: envoie/recoie les métriques par le biais d'une socket zeromq.

**zeromq_aggregator**: fonctionne de la même manière que zeromq à la différence
que la réception des données est non bloquante. Cela permet de forcer la
réception des données par période.

**hdf5**: stocke/lit les données au format hdf5

**rrd**: produit des données au format rrd. Actuellement à chaque itération il
génère l'ensemble du fichier rrd depuis les données reçus pendant cette
itération ainsi que les graphes correspondant.



Dimensionnement
===============

Pour dimensionner de manière large. On considère que :

  - une métrique contient 30 compteurs/entêtes. Tous stockés sur 8 octets.
  - La plateforme exécute en permanence 1000 tâches réparties sur 100 noeuds de
    calcul.
  - Chacune des tâches s'exécutent sur 1/10 de la plateforme (soit 10 noeuds).
  - T est la période de mesure exprimée en secondes.

Trafic réseau
-------------

nous obtenons alors 10 000/T metriques par seconde.  Générant un traffic réseau
de 19/T Mbps.

Espace de stockage
------------------

Pour stocker l'intégralité des données, il faut alors mettre à disposition
70/T To par an.

La taille des données d'une tâche peut s'élever à 200/T Mo/jour

Conclusion
----------

Si la période T est de 60 secondes, colmet génèrerait 0,3 Mbps de traffic sur
le collecteur et 1,2 To de données par an.


Mise en garde / problématiques
==============================

Limites actuelles du principe de mesure avec taskstats
------------------------------------------------------

Il n'y a actuellement pas de prise en compte des taches ayant une vie courte.
Pour pallier à ce problème, si la vitesse de création de processus n'est pas
trop importante, il est serait possible d'utiliser l'interface PROC_EVENT[1] du
noyau linux. Celle-ci permet de d'être notifié par le noyau lors de la
création/destruction d'un processus.

[1] include/linux/cn_proc.h dans les sources linux. Pour l'utiliser, il faut
que CONFIG_PROC_EVENTS et CONFIG_CONNECTOR soient activées dans les options de
la compilation du noyau, c'est le cas pour le noyau 2.6.32 et 3.2.23 dans
debian.


Gouleaux d'étrangelements sur le collecteur
-------------------------------------------

Le gouleau d'étrangelement porte sur :

  - la quantité de données que le collecteur est capable d'ingérer. Il faut
    faire des tests plus approfondis pour valider le fonctionnement de colmet
    sur de grosse infrastructure

  - le nombre de connection tcp ouverte. Par défaut, un système posix sur linux
    limite le nombre de connexions tcp simultannées ouverte à 1024. Ceci
    peut-être limitant sur le collecteur. Pour pallier le problème il est
    possible soit :
        - d'augmenter cette limite
        - d'aggreger les flux par noeud de calcul. Le nombre de socket ouverte
          sur le collecteur correspond au nombre de noeuds de la plateforme
        - utiliser un protocole type multicast utilisé en n-to-1 (dans notre
          cas il s'agit de pgm).


Empreinte mémoire sur les noeuds
--------------------------------

Si trop de job sont mesurés, alors la quantité de mémoire utilisée par colmet
est importante. Pour palier à ce problème, il est possible d'ajouter à colmet
la notion de socket de contrôle afin de pouvoir lui indiquer les tâches à
monitorer sans devoir démarrer de nouveau processus résidant en mémoire.


Accès concurrent écriture/lecture sur un même fichier hdf5
----------------------------------------------------------

Il est précisé sur le site de hdf5, la lecture d'un fichier hdf5 ouvert déjà en
écriture n'est pas garantie en terme de cohérence des données. Cependant par la
pratique, il s'avère que cela ne pose pas de difficulté particulère.

Si nécessaire, il faudra alors, soit répartir l'écriture des données dans un
fichier par tâche de calcul, soit implémenter un mécanisme de préemption
d'accès pour permettre la lecture des données en bloquant les écritures.


Choses à faire
==============

  - Utiliser PROC_EVENT pour surveiller plus finement la création/suppression de tâches dans un groupe de tâches (tgid/pid)
  - Implémenter la notion de socket de contrôle dans colmet pour
        - limiter le nombre de processus colmet exécuter sur un noeud (contrôler la ajout des job à monitorer)
        - faire des requêtes d'extraction des données (cgi, json/yaml, etc.)
  - Implémenter un backend pour transmettre les données au format json/yaml/..., utilisable par l'api de OAR
  - Ajouter une option au backend entrée/sortie hdf5 pour filtrer les données reçues/émises par identifiant de tâches.
